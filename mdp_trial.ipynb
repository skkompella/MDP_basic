{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human', 'ansi']}\n",
    "    \n",
    "    def __init__(self, size=5, render_mode='ansi'):\n",
    "        self.size = size\n",
    "        self.window_size = 512\n",
    "        self.observation_space = spaces.MultiDiscrete([size, size]) # (x, y) coordinates\n",
    "        self.action_space = spaces.Discrete(4) # 0: up, 1: down, 2: left, 3: right (Makes sense for a grid)\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),   # Down\n",
    "            1: np.array([-1, 0]),  # Up\n",
    "            2: np.array([0, 1]),   # Right\n",
    "            3: np.array([0, -1]),  # Left\n",
    "        }\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2) # random location for agent\n",
    "        self._target_location = np.array([self.size - 1, self.size - 1]) # fixed location for target and pit\n",
    "        self._pit_location = np.array([self.size // 2, self.size // 2])\n",
    "\n",
    "        while np.array_equal(self._agent_location, self._target_location) or np.array_equal(self._agent_location, self._pit_location): # while agent is on target or pit change location\n",
    "            self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "        observation = self._get_obs() # get initial observation, info\n",
    "        info = self._get_info()\n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action): # pass state and action to step function\n",
    "        # P section: transition dynamics\n",
    "        direction = self._action_to_direction[action] # get direction from action\n",
    "        self._agent_location = np.clip(self._agent_location + direction, 0, self.size - 1) # change agent location based on action, clip to stay in bounds\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location) or np.array_equal(self._agent_location, self._pit_location)# check if agent is on target or pit\n",
    "\n",
    "        # R: Reward function section\n",
    "        if np.array_equal(self._agent_location, self._target_location):\n",
    "            reward = 10.0\n",
    "        elif np.array_equal(self._agent_location, self._pit_location):\n",
    "            reward = -10.0\n",
    "        else:\n",
    "            reward = -0.1\n",
    "        \n",
    "        observation = self._get_obs() # get new observation, info\n",
    "        info = self._get_info()\n",
    "        truncated = False # no truncation in this environment\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "# Helper/Convenience functions\n",
    "    def _get_obs(self):\n",
    "        return self._agent_location\n",
    "\n",
    "    def _get_info(self):\n",
    "        # Provides the Chebyshev distance to the target, useful for heuristics\n",
    "        return {\"distance\": np.max(np.abs(self._agent_location - self._target_location))}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"ansi\":\n",
    "            grid = np.full((self.size, self.size), \"_\", dtype=str)\n",
    "            grid[tuple(self._agent_location)] = \"A\" # Agent\n",
    "            grid[tuple(self._target_location)] = \"G\" # Goal\n",
    "            grid[tuple(self._pit_location)] = \"P\"   # Pit\n",
    "            print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
    "            print(\"-\" * (2 * self.size))\n",
    "\n",
    "    def close(self):\n",
    "        pass # No resources to close in this simple env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "A _ _ _ _\n",
      "_ _ _ _ G\n",
      "----------\n",
      "Step 1: Taking action 2\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ A _ _ _\n",
      "_ _ _ _ G\n",
      "----------\n",
      "Observation: [3 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 2: Taking action 3\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "A _ _ _ _\n",
      "_ _ _ _ G\n",
      "----------\n",
      "Observation: [3 0], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(4)}\n",
      "Step 3: Taking action 1\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "A _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "----------\n",
      "Observation: [2 0], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(4)}\n",
      "Step 4: Taking action 3\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "A _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "----------\n",
      "Observation: [2 0], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(4)}\n",
      "Step 5: Taking action 3\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "A _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "----------\n",
      "Observation: [2 0], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(4)}\n",
      "Step 6: Taking action 0\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "A _ _ _ _\n",
      "_ _ _ _ G\n",
      "----------\n",
      "Observation: [3 0], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(4)}\n",
      "Step 7: Taking action 0\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "A _ _ _ G\n",
      "----------\n",
      "Observation: [4 0], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(4)}\n",
      "Step 8: Taking action 1\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "A _ _ _ _\n",
      "_ _ _ _ G\n",
      "----------\n",
      "Observation: [3 0], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(4)}\n",
      "Step 9: Taking action 2\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ A _ _ _\n",
      "_ _ _ _ G\n",
      "----------\n",
      "Observation: [3 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 10: Taking action 0\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 11: Taking action 0\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 12: Taking action 2\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ G\n",
      "----------\n",
      "Observation: [4 2], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(2)}\n",
      "Step 13: Taking action 3\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 14: Taking action 0\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 15: Taking action 3\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "A _ _ _ G\n",
      "----------\n",
      "Observation: [4 0], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(4)}\n",
      "Step 16: Taking action 2\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 17: Taking action 2\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ G\n",
      "----------\n",
      "Observation: [4 2], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(2)}\n",
      "Step 18: Taking action 2\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ A G\n",
      "----------\n",
      "Observation: [4 3], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(1)}\n",
      "Step 19: Taking action 3\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ G\n",
      "----------\n",
      "Observation: [4 2], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(2)}\n",
      "Step 20: Taking action 3\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 21: Taking action 2\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ G\n",
      "----------\n",
      "Observation: [4 2], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(2)}\n",
      "Step 22: Taking action 0\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ G\n",
      "----------\n",
      "Observation: [4 2], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(2)}\n",
      "Step 23: Taking action 3\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 24: Taking action 0\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 25: Taking action 2\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ G\n",
      "----------\n",
      "Observation: [4 2], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(2)}\n",
      "Step 26: Taking action 3\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 27: Taking action 0\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 28: Taking action 2\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ G\n",
      "----------\n",
      "Observation: [4 2], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(2)}\n",
      "Step 29: Taking action 3\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 30: Taking action 3\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "A _ _ _ G\n",
      "----------\n",
      "Observation: [4 0], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(4)}\n",
      "Step 31: Taking action 1\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "A _ _ _ _\n",
      "_ _ _ _ G\n",
      "----------\n",
      "Observation: [3 0], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(4)}\n",
      "Step 32: Taking action 2\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ A _ _ _\n",
      "_ _ _ _ G\n",
      "----------\n",
      "Observation: [3 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 33: Taking action 0\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ G\n",
      "----------\n",
      "Observation: [4 1], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(3)}\n",
      "Step 34: Taking action 2\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ G\n",
      "----------\n",
      "Observation: [4 2], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(2)}\n",
      "Step 35: Taking action 0\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ P _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ G\n",
      "----------\n",
      "Observation: [4 2], Reward: -0.1, Terminated: False\n",
      "Info: {'distance': np.int64(2)}\n"
     ]
    }
   ],
   "source": [
    "# Testing the environment\n",
    "env = GridWorldEnv(size=5, render_mode='ansi')\n",
    "observation, info = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(35): # Run for a maximum of 35 steps\n",
    "    action = env.action_space.sample() # Random action\n",
    "    print(f\"Step {i+1}: Taking action {action}\")\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Observation: {observation}, Reward: {reward}, Terminated: {terminated}\")\n",
    "    print(f\"Info: {info}\")\n",
    "\n",
    "    if terminated:\n",
    "        print(\"Episode finished!\")\n",
    "        observation, info = env.reset()\n",
    "        env.render()\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
